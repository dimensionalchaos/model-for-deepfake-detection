{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6826f6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"xhlulu/140k-real-and-fake-faces\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844d82f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Default title text\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Configuration\n",
    "img_size = (64, 64)\n",
    "pca_components = 100\n",
    "dataset_path = \"/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake/train/\"\n",
    "real_faces_dir = os.path.join(dataset_path, \"real\")\n",
    "fake_faces_dir = os.path.join(dataset_path, \"fake\")\n",
    "\n",
    "# Step 1: Load Images\n",
    "def load_images(folder, label, flatten=True):\n",
    "    data, labels = [], []\n",
    "    for filename in os.listdir(folder):\n",
    "        path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, img_size)\n",
    "            data.append(img.flatten() if flatten else img)\n",
    "            labels.append(label)\n",
    "    return data, labels\n",
    "\n",
    "X_real_flat, y_real = load_images(real_faces_dir, 0, flatten=True)\n",
    "X_fake_flat, y_fake = load_images(fake_faces_dir, 1, flatten=True)\n",
    "X_real_img, _ = load_images(real_faces_dir, 0, flatten=False)\n",
    "X_fake_img, _ = load_images(fake_faces_dir, 1, flatten=False)\n",
    "\n",
    "X_flat = np.array(X_real_flat + X_fake_flat)\n",
    "y = np.array(y_real + y_fake)\n",
    "X_img = np.array(X_real_img + X_fake_img)\n",
    "\n",
    "# Split\n",
    "X_train_flat, X_test_flat, y_train, y_test, X_train_img, X_test_img = train_test_split(\n",
    "    X_flat, y, X_img, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: PCA & LDA\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_train_pca = pca.fit_transform(X_train_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "\n",
    "# Debug: Save & load PCA and compare outputs\n",
    "joblib.dump(pca, 'pca_model.pkl')\n",
    "pca_loaded = joblib.load('pca_model.pkl')\n",
    "X_test_pca_original = pca.transform(X_test_flat)\n",
    "X_test_pca_loaded = pca_loaded.transform(X_test_flat)\n",
    "print(\"Max difference in PCA outputs:\", np.abs(X_test_pca_original - X_test_pca_loaded).max())\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "X_train_lda = lda.fit_transform(X_train_pca, y_train)\n",
    "X_test_lda = lda.transform(X_test_pca)\n",
    "\n",
    "# Debug: Save & load LDA and compare outputs\n",
    "joblib.dump(lda, 'lda_model.pkl')\n",
    "lda_loaded = joblib.load('lda_model.pkl')\n",
    "X_test_lda_original = lda.transform(X_test_pca)\n",
    "X_test_lda_loaded = lda_loaded.transform(X_test_pca)\n",
    "print(\"Max difference in LDA outputs:\", np.abs(X_test_lda_original - X_test_lda_loaded).max())\n",
    "\n",
    "# Step 3: LBPH Feature Extraction\n",
    "def extract_lbph_features(img, P=8, R=1, grid_x=8, grid_y=8):\n",
    "    lbp = local_binary_pattern(img, P, R, method='uniform')\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    h, w = img.shape\n",
    "    cell_h, cell_w = h // grid_y, w // grid_x\n",
    "    features = []\n",
    "    for i in range(grid_y):\n",
    "        for j in range(grid_x):\n",
    "            cell = lbp[i*cell_h:(i+1)*cell_h, j*cell_w:(j+1)*cell_w]\n",
    "            hist, _ = np.histogram(cell.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "            hist = hist.astype(\"float\")\n",
    "            hist /= (hist.sum() + 1e-6)\n",
    "            features.extend(hist)\n",
    "    return np.array(features)\n",
    "\n",
    "X_train_lbph = np.array([extract_lbph_features(img) for img in X_train_img])\n",
    "X_test_lbph = np.array([extract_lbph_features(img) for img in X_test_img])\n",
    "\n",
    "# Step 4: Feature Fusion\n",
    "X_train_fused = np.concatenate((X_train_lbph, X_train_lda), axis=1)\n",
    "X_test_fused = np.concatenate((X_test_lbph, X_test_lda), axis=1)\n",
    "\n",
    "# Reshape for CNN\n",
    "X_train_cnn = X_train_fused[..., np.newaxis]\n",
    "X_test_cnn = X_test_fused[..., np.newaxis]\n",
    "\n",
    "# Step 5: CNN Model\n",
    "model = Sequential([\n",
    "    InputLayer(input_shape=(X_train_fused.shape[1], 1)),\n",
    "    Conv1D(32, 3, activation='relu', padding='same'),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(X_train_cnn, y_train, validation_data=(X_test_cnn, y_test), epochs=50, batch_size=32, callbacks=[early_stop])\n",
    "model.save(\"fused_cnn_model.h5\")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test_cnn, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Step 6: Prediction Function for Individual Images\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import local_binary_pattern\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Assuming img_size is defined globally as in your training code\n",
    "img_size = (64, 64)\n",
    "\n",
    "def extract_lbph_features(img, P=8, R=1, grid_x=8, grid_y=8):\n",
    "    \"\"\"\n",
    "    Extracts Local Binary Pattern Histogram features from a given image.\n",
    "    \"\"\"\n",
    "    lbp = local_binary_pattern(img, P, R, method='uniform')\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    h, w = img.shape\n",
    "    cell_h, cell_w = h // grid_y, w // grid_x\n",
    "    features = []\n",
    "    for i in range(grid_y):\n",
    "        for j in range(grid_x):\n",
    "            cell = lbp[i*cell_h:(i+1)*cell_h, j*cell_w:(j+1)*cell_w]\n",
    "            hist, _ = np.histogram(cell.ravel(), bins=n_bins, range=(0, n_bins))\n",
    "            hist = hist.astype(\"float\")\n",
    "            hist /= (hist.sum() + 1e-6)\n",
    "            features.extend(hist)\n",
    "    return np.array(features)\n",
    "\n",
    "def predict_image(img_path):\n",
    "    # Load image in grayscale and resize\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not found or unreadable.\")\n",
    "    img = cv2.resize(img, img_size)\n",
    "\n",
    "    # Preprocess image for PCA transform: flatten and reshape to (1, -1)\n",
    "    img_flat = img.flatten().reshape(1, -1)\n",
    "\n",
    "    # Load pre-trained models (PCA, LDA, CNN)\n",
    "    pca = joblib.load(\"pca_model.pkl\")\n",
    "    lda = joblib.load(\"lda_model.pkl\")\n",
    "    cnn = load_model(\"fused_cnn_model.h5\")\n",
    "\n",
    "    # Apply PCA then LDA transformation\n",
    "    img_pca = pca.transform(img_flat)\n",
    "    img_lda = lda.transform(img_pca)\n",
    "\n",
    "    # If img_lda is one-dimensional, reshape it to two dimensions\n",
    "    if len(img_lda.shape) == 1:\n",
    "        img_lda = img_lda.reshape(1, -1)\n",
    "\n",
    "    # Extract LBPH features from the resized grayscale image\n",
    "    lbph_features = extract_lbph_features(img)  # returns a 1D vector\n",
    "    lbph_features = lbph_features.reshape(1, -1)  # reshape to (1, feature_length)\n",
    "\n",
    "    # Fuse LBPH and LDA features: concatenate along the feature axis.\n",
    "    fused_features = np.concatenate((lbph_features, img_lda), axis=1)\n",
    "\n",
    "    # Reshape for CNN input: add a channel dimension so shape becomes (1, num_features, 1)\n",
    "    fused_features = fused_features[..., np.newaxis]\n",
    "\n",
    "    # Predict using the CNN model\n",
    "    prediction = cnn.predict(fused_features)\n",
    "    print(\"Prediction (probability):\", prediction[0][0])\n",
    "    return prediction[0][0]\n",
    "\n",
    "# Example usage:\n",
    "# predict_image(\"path/to/your/image.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
